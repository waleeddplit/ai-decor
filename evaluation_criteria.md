# 🧾 Art.Decor.AI – Evaluation Criteria

Comprehensive evaluation framework for assessing the Art.Decor.AI project across functionality, AI design, teamwork, UX, and architecture quality.

---

## 1️⃣ Functionality (25%)

| Sub-criteria                     | Description                                                          | Scoring Guide |
| -------------------------------- | -------------------------------------------------------------------- | ------------- |
| **Core Features Implementation** | Upload, analyze, recommend, and explain features work end-to-end.    | 0–10          |
| **System Reliability**           | Application runs smoothly with no major crashes or broken API links. | 0–5           |
| **Integration Accuracy**         | AI agents and backend orchestrator communicate correctly.            | 0–5           |
| **Performance & Latency**        | App responds within 3 seconds for normal queries.                    | 0–5           |

> ✅ _Goal:_ A stable, feature-complete prototype demonstrating an end-to-end AI pipeline.

---

## 2️⃣ User Experience (20%)

| Sub-criteria               | Description                                                            | Scoring Guide |
| -------------------------- | ---------------------------------------------------------------------- | ------------- |
| **Interface Design**       | Modern, minimal UI; clean layout; responsive across devices.           | 0–5           |
| **Ease of Use**            | Intuitive navigation, minimal friction in using input/output features. | 0–5           |
| **Multimodal Interaction** | Smooth support for photo, text, and voice input.                       | 0–5           |
| **Aesthetic Consistency**  | Typography, spacing, and color choices align with décor theme.         | 0–5           |

> 🎨 _Goal:_ A professional, elegant user experience that reflects a real AI design advisor.

---

## 3️⃣ AI & Technical Innovation (25%)

| Sub-criteria                      | Description                                                                       | Scoring Guide |
| --------------------------------- | --------------------------------------------------------------------------------- | ------------- |
| **Model Selection & Integration** | Correct use of YOLOv8, DINOv2/CLIP, and LLaVA or Llama Vision.                    | 0–5           |
| **Embedding & Retrieval Quality** | FAISS search retrieves visually and stylistically relevant artworks.              | 0–5           |
| **AI Reasoning Depth**            | Generated explanations (“Why These Choices Work”) are coherent and context-aware. | 0–5           |
| **Agent Collaboration**           | Vision-Match, Geo-Finder, and Trend-Intel Agents coordinate effectively.          | 0–5           |
| **Efficiency & Optimization**     | Good inference speed, caching, and minimal compute waste.                         | 0–5           |

> 🤖 _Goal:_ Demonstrate multimodal reasoning and technically sound AI orchestration.

---

## 4️⃣ Data & Personalization (10%)

| Sub-criteria              | Description                                                          | Scoring Guide |
| ------------------------- | -------------------------------------------------------------------- | ------------- |
| **Dataset Preparation**   | Clean, diverse art/room dataset with meaningful tags and embeddings. | 0–4           |
| **Retrieval Accuracy**    | Matching score consistency across multiple queries.                  | 0–3           |
| **Personalization Logic** | User profile effectively influences future recommendations.          | 0–3           |

> 🧠 _Goal:_ Ensure data-driven adaptability and learning behavior.

---

## 5️⃣ System Architecture & Code Quality (10%)

| Sub-criteria              | Description                                                 | Scoring Guide |
| ------------------------- | ----------------------------------------------------------- | ------------- |
| **Architecture Clarity**  | Clean separation between frontend, backend, and AI modules. | 0–4           |
| **Code Quality**          | Well-structured code, naming conventions, modularity.       | 0–3           |
| **Documentation & Setup** | Clear README, setup guide, and environment instructions.    | 0–3           |

> 🧩 _Goal:_ Codebase that’s scalable, readable, and easily maintainable.

---

## 6️⃣ Presentation & Communication (5%)

| Sub-criteria               | Description                                                      | Scoring Guide |
| -------------------------- | ---------------------------------------------------------------- | ------------- |
| **Demo Quality**           | Smooth live or recorded walkthrough demonstrating workflow.      | 0–2           |
| **Presentation Storyline** | Clear articulation of problem → solution → tech stack → outcome. | 0–2           |
| **Visual Clarity**         | Slides, diagrams, and visuals support narrative effectively.     | 0–1           |

> 💬 _Goal:_ Tell a compelling, easy-to-follow story of innovation and teamwork.

---

## 7️⃣ Teamwork & Project Understanding (5%)

| Sub-criteria               | Description                                                               | Scoring Guide |
| -------------------------- | ------------------------------------------------------------------------- | ------------- |
| **Balanced Contribution**  | All team members participate actively (frontend, backend, AI, data).      | 0–2           |
| **Collaborative Workflow** | Effective coordination via Git commits, issue tracking, and code reviews. | 0–1           |
| **Project Comprehension**  | Each member can explain the system flow, models used, and architecture.   | 0–2           |

> 🤝 _Goal:_ A cohesive team demonstrating shared ownership and full technical understanding of the system.

---

## 🧮 Scoring Summary

| Category                     | Weight | Max Points |
| ---------------------------- | ------ | ---------- |
| Functionality                | 25%    | 25         |
| User Experience              | 20%    | 20         |
| AI & Technical Innovation    | 25%    | 25         |
| Data & Personalization       | 10%    | 10         |
| Architecture & Code Quality  | 10%    | 10         |
| Presentation & Communication | 5%     | 5          |
| Teamwork & Understanding     | 5%     | **5**      |
| **Total**                    | —      | **100**    |

---

### 🏆 Evaluation Levels

| Score Range | Rating         | Description                                                 |
| ----------- | -------------- | ----------------------------------------------------------- |
| **90–100**  | ⭐ Outstanding | Fully functional, innovative, and well-coordinated project. |
| **75–89**   | 🟢 Strong      | Robust implementation with room for refinement.             |
| **60–74**   | 🟡 Good        | Functional with some gaps in UX or AI accuracy.             |
| **< 60**    | 🔴 Needs Work  | Partial or inconsistent implementation.                     |

---

### 📈 Bonus Points (Up to +5)

| Category                        | Description                                     |
| ------------------------------- | ----------------------------------------------- |
| 🎤 **Voice Output Integration** | Natural TTS responses for AI explanations.      |
| 🪄 **AR Preview Mode**          | Mockup of art placement via WebAR.              |
| 📊 **Analytics Dashboard**      | Track user engagement or recommendation trends. |

---

**Evaluator Note:**  
Each category should be graded with evidence from live demo, repo inspection, and team Q&A to assess understanding and collaboration depth.

---
